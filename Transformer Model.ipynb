{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4b8509-4466-4199-8e32-503265ab1e30",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9df9a-2b8f-4f3d-9390-0f6cd2ca4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9bf2ff-d974-46b6-bcc5-4488073d4d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0405acf0-afbe-4b89-bdfd-d4872dfbed9e",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd609d5-5059-4621-bdaf-72873579396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load data from the output file #####\n",
    "path_output = '../Transformer/data/output/data_sequence_pm.pt'\n",
    "\n",
    "X_train, y_train, X_test, y_test = torch.load(path_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67256dcd-31bd-4e02-a627-02237949678d",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a7655-30ce-4827-80da-d9961c25f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### sampling: smote #####\n",
    "X_train = X_train.cpu()\n",
    "y_train = y_train.cpu()\n",
    "\n",
    "sample_size, window, feature_size = X_train.shape\n",
    "\n",
    "X_train = X_train.reshape(-1, window * feature_size)  # shape: [num_samples, num_timesteps * num_features]\n",
    "\n",
    "smote = SMOTE(sampling_strategy = 1.0, random_state = 0)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(-1, window, feature_size)\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1a0a0-a4af-4c9e-8312-64033855f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### create datasets #####\n",
    "train_dataset, test_dataset = TensorDataset(X_train, y_train), TensorDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "##### create dataLoaders #####\n",
    "train_loader, test_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True), DataLoader(test_dataset, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864d8a2-a3f5-4742-87bc-909858e837ff",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e5755b-13fc-43d3-8e96-85ef06558111",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe579c7a-a437-447b-9019-565638eb7041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vector(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.weights_linear = nn.Parameter(torch.randn(self.seq_len), requires_grad=True)\n",
    "        self.bias_linear = nn.Parameter(torch.randn(self.seq_len), requires_grad=True)\n",
    "        \n",
    "        self.weights_periodic = nn.Parameter(torch.randn(self.seq_len), requires_grad=True)\n",
    "        self.bias_periodic = nn.Parameter(torch.randn(self.seq_len), requires_grad=True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x shape: [batch_size, seq_len, num_features]\n",
    "        x = torch.mean(x[:, :, :], dim=-1)  # reducing last dimension, shape: [batch_size, seq_len]\n",
    "        \n",
    "        time_linear = self.weights_linear * x + self.bias_linear  # shape: [batch_size, seq_len]\n",
    "        time_linear = time_linear.unsqueeze(-1) # shape: [batch_size, seq_len, 1]\n",
    "        \n",
    "        time_periodic = torch.sin(x * self.weights_periodic + self.bias_periodic)  # shape: [batch_size, seq_len]\n",
    "        time_periodic = time_periodic.unsqueeze(-1) # shape: [batch_size, seq_len, 1]\n",
    "        \n",
    "        \n",
    "        return torch.cat([time_linear, time_periodic], dim=-1) # concatenating linear and periodic components, shape: [batch_size, seq_len, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632900bb-44ec-4501-a26c-ca6a76417116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_k, d_v, input_features):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        \n",
    "        self.d_k = d_k\n",
    "        \n",
    "        self.query = nn.Linear(input_features, d_k)\n",
    "        self.key = nn.Linear(input_features, d_k)\n",
    "        self.value = nn.Linear(input_features, d_v)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        q = self.query(inputs[0])  # [batch_size, seq_len, d_k]\n",
    "        k = self.key(inputs[1])    # [batch_size, seq_len, d_k]\n",
    "        v = self.value(inputs[2])  # [batch_size, seq_len, d_v]\n",
    "        \n",
    "        attn_weights = torch.bmm(q, k.transpose(1, 2)) / np.sqrt(self.d_k)\n",
    "        attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        attn_out = torch.bmm(attn_weights, v)  # [batch_size, seq_len, d_v]\n",
    "        \n",
    "        \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1655e-ac63-45d6-8fa8-1bb4a3a8640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_k, d_v, n_heads, input_features):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.attn_heads = nn.ModuleList([SingleAttention(d_k, d_v, input_features) for _ in range(n_heads)])\n",
    "        self.linear = nn.Linear(n_heads * d_v, input_features)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        attn = [head(inputs) for head in self.attn_heads]  # [batch_size, seq_len, d_v] each\n",
    "        concat_attn = torch.cat(attn, dim=-1)  # [batch_size, seq_len, n_heads * d_v]\n",
    "        multi_linear = self.linear(concat_attn)  # [batch_size, seq_len, input_features]\n",
    "        \n",
    "        return multi_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96bd34c-e361-41b5-ad01-65b1cfa58deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, input_features, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        input_features = input_features + 2\n",
    "        \n",
    "        self.attn_multi = MultiAttention(d_k, d_v, n_heads, input_features)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.attn_normalize = nn.LayerNorm(normalized_shape=(input_features,))\n",
    "        \n",
    "        self.ff_conv1D_1 = nn.Conv1d(in_channels=input_features, out_channels=ff_dim, kernel_size=1)\n",
    "        self.ff_conv1D_2 = nn.Conv1d(in_channels=ff_dim, out_channels=input_features, kernel_size=1)\n",
    "        \n",
    "        self.ff_dropout = nn.Dropout(dropout)\n",
    "        self.ff_normalize = nn.LayerNorm(normalized_shape=(input_features,))\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        attn_layer = self.attn_multi(inputs)  # [batch_size, seq_len, input_features]\n",
    "        attn_layer = self.attn_dropout(attn_layer)\n",
    "        attn_layer = self.attn_normalize(attn_layer + inputs[0])\n",
    "        attn_layer = attn_layer.transpose(1, 2)  # [batch_size, input_features, seq_len]\n",
    "        \n",
    "        ff_layer = self.ff_conv1D_1(attn_layer)  # [batch_size, ff_dim, seq_len]\n",
    "        ff_layer = self.ff_conv1D_2(ff_layer)  # [batch_size, input_features, seq_len]\n",
    "        ff_layer = self.ff_dropout(ff_layer)\n",
    "        ff_layer = self.ff_normalize(ff_layer.transpose(1, 2) + attn_layer.transpose(1, 2))  # [batch_size, seq_len, input_features]\n",
    "        \n",
    "        \n",
    "        return ff_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c865e-ebdd-4fbf-8f3f-a85bd309931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, d_k, d_v, n_heads, ff_dim, input_features, num_classes=1, dropout=0.1):\n",
    "        super(TimeTransformer, self).__init__()\n",
    "        \n",
    "        self.time_embedding = Time2Vector(seq_len)\n",
    "        \n",
    "        self.attn_layer1 = TransformerEncoderLayer(d_k, d_v, n_heads, ff_dim, input_features, dropout)\n",
    "        self.attn_layer2 = TransformerEncoderLayer(d_k, d_v, n_heads, ff_dim, input_features, dropout)\n",
    "        self.attn_layer3 = TransformerEncoderLayer(d_k, d_v, n_heads, ff_dim, input_features, dropout)\n",
    "        self.attn_layer4 = TransformerEncoderLayer(d_k, d_v, n_heads, ff_dim, input_features, dropout)\n",
    "        \n",
    "        self.global_avg_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_features + 2, 16)\n",
    "        self.fc2 = nn.Linear(16, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        time_embed = self.time_embedding(x)  # [batch_size, seq_len, 2]\n",
    "        x = torch.cat([x, time_embed], dim=-1)  # [batch_size, seq_len, input_features + 2]\n",
    "\n",
    "        x = self.attn_layer1((x, x, x))\n",
    "        x = self.attn_layer2((x, x, x))\n",
    "        x = self.attn_layer3((x, x, x))\n",
    "        x = self.attn_layer4((x, x, x))\n",
    "        \n",
    "        x = self.global_avg_pooling(x.transpose(1, 2))  # [batch_size, input_features, 1]\n",
    "        x = x.squeeze(-1)  # [batch_size, input_features]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        out = torch.sigmoid(self.fc2(x))  # apply sigmoid here\n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e223a34-8da4-4ec9-8021-8f54878f9a5a",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f85d3-ea0e-45b5-81d7-783152f56188",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeTransformer(\n",
    "    seq_len=window,             \n",
    "    input_features=feature_size,       \n",
    "    \n",
    "    d_k=128,\n",
    "    d_v=128,\n",
    "    n_heads=5,              # number of heads in multiheadattention\n",
    "    ff_dim=64,              # dimension of feedforward network\n",
    "    \n",
    "    num_classes=1           # binary classification\n",
    ")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00af9d5-e803-452e-bd16-1e334d00dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d59d64-f39c-4276-8756-8f865b639148",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### function to determine recall and far #####\n",
    "def get_metrics(y_true, y_pred):\n",
    "\n",
    "    y_pred = torch.round(y_pred).cpu().numpy()\n",
    "    y_true = y_true.cpu().numpy()\n",
    "\n",
    "\n",
    "    c_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    tn, fp, fn, tp = c_matrix.ravel()\n",
    "\n",
    "    # calculate recall (sensitivity)\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "    # calculate false alarm rate (false positive rate)\n",
    "    false_alarm_rate = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "\n",
    "    \n",
    "    return recall, false_alarm_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e1a95-257a-44c3-b0f2-a8933ce138a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### training #####\n",
    "num_epochs = 25\n",
    "\n",
    "optimum_validation_recall = 0.7\n",
    "optimum_validation_far = 0.4\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # training phase\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # lists to store true labels and predictions for each epoch\n",
    "    y_true = []; y_pred = []\n",
    "\n",
    "    # wrap train_loader with tqdm for a progress bar\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    \n",
    "    for batch_idx, (X, y) in progress_bar:\n",
    "        \n",
    "        X = X.to(device); y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        \n",
    "        loss = criterion(outputs, y.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # update running loss and batch count\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # accumulate true and predicted values\n",
    "        y_true.append(y.detach())\n",
    "        y_pred.append(outputs.detach())\n",
    "\n",
    "        \n",
    "        \n",
    "    progress_bar.close()\n",
    "        \n",
    "    # convert lists to tensors\n",
    "    y_true = torch.cat(y_true); y_pred = torch.cat(y_pred)\n",
    "\n",
    "    train_recall, train_far = get_metrics(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()\n",
    "    \n",
    "    validation_loss = 0.0\n",
    "    validation_batches = 0\n",
    "    \n",
    "    y_true = []; y_pred = []\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for X, y in test_loader:\n",
    "            \n",
    "            X = X.to(device); y = y.to(device)\n",
    "\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y.float())\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "            validation_batches += 1\n",
    "\n",
    "            y_true.append(y.detach())\n",
    "            y_pred.append(outputs.detach())\n",
    "\n",
    "            \n",
    "            \n",
    "    # calculate validation metrics\n",
    "    y_true = torch.cat(y_true)\n",
    "    y_pred = torch.cat(y_pred)\n",
    "    \n",
    "    validation_recall, validation_far = get_metrics(y_true, y_pred)\n",
    "\n",
    "    validation_loss = validation_loss / validation_batches\n",
    "\n",
    "\n",
    "    \n",
    "    print('Reports...........')\n",
    "    \n",
    "    print(f'Training ---> Loss: {epoch_loss / num_batches:.4f}, Recall: {train_recall:.4f}, False Alarm Rate: {train_far:.4f}')\n",
    "    print(f'Validity ---> Loss: {validation_loss:.4f}, Recall: {validation_recall:.4f}, False Alarm Rate: {validation_far:.4f}')\n",
    "\n",
    "    \n",
    "    \n",
    "    # check if the recall has improved\n",
    "    \n",
    "    if (validation_recall > optimum_validation_recall) and (validation_far < optimum_validation_far):\n",
    "        \n",
    "        optimum_validation_recall = validation_recall\n",
    "        optimum_validation_far = validation_far\n",
    "        \n",
    "        print('\\nValidation Recall Improved, Saving Model...')\n",
    "        \n",
    "        torch.save(model.state_dict(), f'../Transformer/model/{epoch + 1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b640985-f1cb-44fa-bda2-9e2c73785c44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ##### load the best model after training is complete #####\n",
    "# model.load_state_dict(torch.load('../Transformer/model/.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf1a48-c8cf-4d96-ac75-936dbf57d65e",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa8a97-e7d5-4240-ab38-bce3bf470155",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### function to determine recall and far #####\n",
    "def get_metrics(y_true, y_pred):\n",
    "\n",
    "    y_pred = torch.round(y_pred).cpu().numpy()\n",
    "    y_true = y_true.cpu().numpy()\n",
    "\n",
    "\n",
    "    c_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    tn, fp, fn, tp = c_matrix.ravel()\n",
    "\n",
    "    # calculate recall (sensitivity)\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "    # calculate false alarm rate (false positive rate)\n",
    "    false_alarm_rate = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "\n",
    "    \n",
    "    return c_matrix, recall, false_alarm_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f17cc-fd51-4a4a-a0da-60f5004618bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### evaluate the model\n",
    "model.eval()\n",
    "\n",
    "y_true = []; y_pred = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for X, y in test_loader:\n",
    "        \n",
    "        X = X.to(device); y = y.to(device)\n",
    "\n",
    "        outputs = model(X)\n",
    "        \n",
    "        y_pred.extend(outputs)\n",
    "        y_true.extend(y)\n",
    "\n",
    "        \n",
    "        \n",
    "# convert lists to tensors\n",
    "y_true = torch.stack(y_true)\n",
    "y_pred = torch.stack(y_pred)\n",
    "\n",
    "# calculate metrics\n",
    "c_matrix, recall, false_alarm_rate = get_metrics(y_true, y_pred)\n",
    "\n",
    "\n",
    "print(c_matrix)\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'False Alarm Rate: {false_alarm_rate:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
